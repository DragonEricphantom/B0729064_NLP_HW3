{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"B0729064_HW03.ipynb","provenance":[],"collapsed_sections":[],"mount_file_id":"1JFitwrzjMk_I_vzEGPLZpsjZ2jU20ZtB","authorship_tag":"ABX9TyPAqNgj0VUEvUQmPzlhbxRT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"KX4c6gt5JpLh","executionInfo":{"status":"error","timestamp":1629098164458,"user_tz":-480,"elapsed":54538,"user":{"displayName":"楊永川","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggx1XlzmI8P9pcajCEBcheu4kz3U9OvvaM8dTnb=s64","userId":"08079793556666005334"}},"outputId":"e8ca97fe-39c0-4a8b-e0f9-826175b02d63"},"source":["from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.feature_extraction.text import CountVectorizer\n","import pandas as pd   \n","import numpy as np   \n","import jieba\n","\n","# max\n","# score\n","# WEIGHT\n","jieba_results = []\n","new_data_onelabe = []\n","def TF_IDF():\n","  stopword_list_ch = []\n","  manual_stop_list = [',','.','*',':',';','#','@','!','%','^','&','+','=','~','『 ','\\\\n','{','}','-','|','(',')',',','˙','..','/','...','$','\\r','\\n','\\t',' ','\\r\\n','▼','｜','【','】','[',']','「','」','★','▎','↘','◤']\n","  for w in manual_stop_list:\n","      stopword_list_ch.append(w)\n","  stopword_list_ch\n","  df_article = pd.read_csv('movies.csv').astype(str)\n","\n","  for sent in df_article['劇情介紹']:\n","      w_string = str()\n","      ws = jieba.cut(sent, cut_all=False) # 精確模式\n","      for w in ws:\n","          if w not in stopword_list_ch:\n","              w_string += w + ' '\n","      jieba_results.append(w_string)\n","  df_article['sent_jieba'] = jieba_results\n","  vectorizer = CountVectorizer(stop_words = manual_stop_list)\n","  transformer = TfidfTransformer()\n","  X = vectorizer.fit_transform(df_article.sent_jieba)\n","  tfidf = transformer.fit_transform(X)\n","  TFIDF =  tfidf\n","  print(type(tfidf))\n","  weight = tfidf.toarray()\n","  WEIGHT = weight\n","  print(weight.shape)\n","  print(tfidf)\n","  print(weight[0])\n","  return TFIDF \n","\n","def Knn(TFIDF):\n","  TFIDF = set(TFIDF)\n","  print(\"in knn\")\n","  for r in range(100, 1000):\n","    train_data, test_data, train_label, test_label = train_test_split(WEIGHT,labels,test_size=500)\n","    knn=KNeighborsClassifier(n_neighbors=17,p=2,weights='distance')\n","    knn.fit(train_data,train_label)\n","    knn.predict(test_data)\n","    knn.score(test_data,test_label)\n","    print(knn.score)\n","    len(train_data)\n","\n","    accuracy = []\n","    knn = KNeighborsClassifier(n_neighbors=k) \n","    knn.fit(train_data, train_label)                 \n","    y_pred = knn.predict(test_data)              \n","    accuracy.append(metrics.accuracy_score(test_label, y_pred)) \n","\n","\n","if __name__ == '__main__':\n","  TFIDF = []\n","  TFIDF = TF_IDF()\n","  Knn(TFIDF)\n","\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<class 'scipy.sparse.csr.csr_matrix'>\n","(7250, 182701)\n","  (0, 177942)\t0.06118142051548295\n","  (0, 173030)\t0.055110586877196756\n","  (0, 172132)\t0.060316890484324724\n","  (0, 171770)\t0.03598064737288674\n","  (0, 169760)\t0.05288628253425227\n","  (0, 168567)\t0.07648914661794852\n","  (0, 167848)\t0.05931920447135642\n","  (0, 166215)\t0.06353487824045198\n","  (0, 165621)\t0.09918977804670102\n","  (0, 165606)\t0.03609763147493517\n","  (0, 165522)\t0.06175620754979384\n","  (0, 164097)\t0.10727590611351295\n","  (0, 163186)\t0.10254582441839914\n","  (0, 163066)\t0.06904533582821538\n","  (0, 161811)\t0.05265502069411201\n","  (0, 160878)\t0.04682616819737155\n","  (0, 160592)\t0.08041437010951327\n","  (0, 159935)\t0.09658662624313706\n","  (0, 157982)\t0.08972961465647342\n","  (0, 157955)\t0.048783931020623506\n","  (0, 157464)\t0.06313024287320958\n","  (0, 155914)\t0.07568428841439945\n","  (0, 155907)\t0.23074021468364309\n","  (0, 153651)\t0.07121637094002148\n","  (0, 153154)\t0.08605605940979981\n","  :\t:\n","  (7249, 16180)\t0.028713578717252778\n","  (7249, 15957)\t0.07711768969302607\n","  (7249, 15787)\t0.02947132470487647\n","  (7249, 15722)\t0.025888787022414907\n","  (7249, 15581)\t0.039186216226709626\n","  (7249, 15462)\t0.019954180961510335\n","  (7249, 14137)\t0.0551017287872795\n","  (7249, 13917)\t0.02747828754977437\n","  (7249, 13897)\t0.03644445403440959\n","  (7249, 13886)\t0.05973776659215979\n","  (7249, 13703)\t0.024185117071098394\n","  (7249, 13696)\t0.05652913898304474\n","  (7249, 13562)\t0.022134339195219874\n","  (7249, 13494)\t0.03710065612530525\n","  (7249, 13364)\t0.04179244455776233\n","  (7249, 13049)\t0.025039401034968715\n","  (7249, 13016)\t0.04081504348511135\n","  (7249, 12833)\t0.03988343214249071\n","  (7249, 12747)\t0.011216448396330838\n","  (7249, 12738)\t0.017575760848255257\n","  (7249, 12360)\t0.10379925489726255\n","  (7249, 8581)\t0.0551017287872795\n","  (7249, 6124)\t0.1653051863618385\n","  (7249, 560)\t0.11947553318431958\n","  (7249, 559)\t0.026802566664801623\n","[0. 0. 0. ... 0. 0. 0.]\n"],"name":"stdout"},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-2bdc3eed2874>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     61\u001b[0m   \u001b[0mTFIDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m   \u001b[0mTFIDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTF_IDF\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m   \u001b[0mKnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTFIDF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-3-2bdc3eed2874>\u001b[0m in \u001b[0;36mKnn\u001b[0;34m(TFIDF)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mKnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTFIDF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m   \u001b[0mTFIDF\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTFIDF\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"in knn\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: unhashable type: 'csr_matrix'"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xhyKbotxg8xC","outputId":"32ade168-7aca-498f-e47c-24a0f798a314"},"source":["from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.feature_extraction.text import CountVectorizer\n","import pandas as pd   \n","import numpy as np   \n","import jieba\n","TFIDF = []\n","\n","jieba_results = []\n","new_data_onelabe = []\n","\n","stopword_list_ch = []\n","manual_stop_list = [',','.','*',':',';','#','@','!','%','^','&','+','=','~','『 ','\\\\n','{','}','-','|','(',')',',','˙','..','/','...','$','\\r','\\n','\\t',' ','\\r\\n','▼','｜','【','】','[',']','「','」','★','▎','↘','◤']\n","for w in manual_stop_list:\n","    stopword_list_ch.append(w)\n","stopword_list_ch\n","df_article = pd.read_csv('movies.csv').astype(str)\n","\n","for sent in df_article['劇情介紹']:\n","    w_string = str()\n","    ws = jieba.cut(sent, cut_all=False) # 精確模式\n","    for w in ws:\n","        if w not in stopword_list_ch:\n","            w_string += w + ' '\n","    jieba_results.append(w_string)\n","df_article['sent_jieba'] = jieba_results\n","vectorizer = CountVectorizer(stop_words = manual_stop_list)\n","transformer = TfidfTransformer()\n","X = vectorizer.fit_transform(df_article.sent_jieba)\n","tfidf = transformer.fit_transform(X)\n","TFIDF =  tfidf\n","max = 0\n","tfarray = []\n","print(type(tfidf))\n","weight = tfidf.toarray()\n","WEIGHT = weight\n","print(weight.shape)\n","# print(tfidf)\n","# print(tfarray)\n","nonzero=tfidf.nonzero()\n","print(type(nonzero))\n","print(nonzero)\n","print(tfidf[0])\n","print(\"\\n\")\n","print(tfidf[1])\n","print(\"\\n\")\n","print(tfidf[2])\n","\n","# class obj:\n","#   def __init__(self):\n","#       self.key=0\n","#       self.weight=0.0\n","# k=0 #k用来记录是不是一条记录结束了\n","# max = 0\n","# lis=[]\n","# weight\n","# gather=[]\n","# p=-1 #p用来计数，每走一遍循环+1\n","# for i in nonzero[0]:\n","#     p=p+1\n","#     # print(\"i in nonzero and i = \")\n","#     print(i)\n","#     if k==i:\n","#         a=obj()\n","#         a.key=nonzero[1][p]#这个词的字典编号就是它属于第几列\n","#         a.weight=tfidf[i,nonzero[1][p]]\n","#         print(\"a.weight = \")\n","#         print(a.weight)\n","#         lis.append(a)\n","#     else:\n","#         lis.sort(key=lambda obj: obj.weight, reverse=True)#对链表内为类对象的排序\n","#         #print(lis)\n","#         gather.append(lis)\n","#         while k < i:\n","#             k=k+1\n","#         lis=[]\n","#         a=obj()\n","#         a.key=nonzero[1][p]\n","#         a.weight=tfidf[i,nonzero[1][p]]\n","#         lis.append(a)\n","# gather.append(lis)\n","# myexcel = xlwt.Workbook()\n","# sheet = myexcel.add_sheet('sheet')\n","# #si,sj表示输出到第几行第几列\n","# si=-1\n","# sj=-1\n","# for i in gather:\n","#     si=si+1\n","#     for j in i:\n","#         sj=sj+1\n","#         sheet.write(si,sj,str(j.key))\n","#     while sj<=76:\n","#         sj=sj+1\n","#         sheet.write(si,sj,'-1')#要是没有那么多词组就用-1代替\n","#     sj=-1\n","# myexcel.save(\"attribute76.xls\")\n"],"execution_count":null,"outputs":[{"output_type":"stream","text":["<class 'scipy.sparse.csr.csr_matrix'>\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YJCAWlKmL3r0"},"source":["\n","import re\n","import pandas as pd\n","from sklearn.feature_extraction.text import CountVectorizer\n","import jieba\n","import jieba.analyse\n","from sklearn.feature_extraction.text import TfidfTransformer\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn import metrics\n","from sklearn.model_selection import train_test_split    #分割資料集\n","\n","def cutval(df):\n","    topcut = []\n","    for d in df['劇情介紹']:\n","        top = jieba.analyse.extract_tags(d,topK=2)\n","        topcut.append(top)\n","\n","    typecut = []\n","    for d in df['oneLabel']:\n","        # ch =re.compile(\"[\\u4e00-\\u9fa5]\")\n","        # seg_word =  \"\".join(ch.findall(d))\n","        # top = jieba.lcut(seg_word)\n","        print(d)\n","        typecut.append(d)\n","\n","    namecut = []\n","    for d in df['中文名稱']:\n","        ch =re.compile(\"[\\u4e00-\\u9fa5]\")\n","        name =  \"\".join(ch.findall(d))\n","        namecut.append(name)\n","\n","    data = { 'type':typecut ,'name':namecut,'article':topcut}\n","    df1 = pd.DataFrame(data)\n","\n","    df1['type'] = df1['type'].apply(lambda x :  str(x)[1:-2].replace(\"'\",\"\").replace(\",\",\"\"))\n","    df1['article'] = df1['article'].apply(lambda x :  str(x)[1:-2].replace(\"'\",\"\").replace(\",\",\"\"))\n","    df1['key'] = df1['name'].astype(str)+' '+df1['type'].astype(str)+' '+df1['article'].astype(str)\n","    return df1\n","\n","\n","def count(df):\n","    vectorizer = CountVectorizer()\n","    X = vectorizer.fit_transform(df['key'])\n","    tfidf = TfidfTransformer() \n","    tf=tfidf.fit_transform(X)\n","    word = vectorizer.get_feature_names() #詞袋 \n","    \n","    return tf\n","\n","def knn(X,Y):\n","    X_train, X_test, y_train, y_test = train_test_split(X, Y['type'].str[0:2], test_size=0.07659)\n","    clf=KNeighborsClassifier(n_neighbors=51)\n","    clf.fit(X_train,y_train)\n","\n","    y_pred = clf.predict(X_test)    # 預測模型 \n","    y_test=y_test.values\n","\n","    print(\"精準度：\",metrics.accuracy_score(y_test, y_pred))\n","\n","df = pd.read_csv('movies.csv')\n","df = df.drop(labels=['Unnamed: 0'],axis='columns')\n","df1 = cutval(df) \n","tf = count(df1)\n","knn(tf,df1)"],"execution_count":null,"outputs":[]}]}